{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1631120440463,
     "user": {
      "displayName": "Dân Phạm",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01439254586035670376"
     },
     "user_tz": -420
    },
    "id": "dbVR-4SpkS54",
    "outputId": "231f5d62-593a-4511-b0c3-5622315f0105"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/anlab/Downloads/imageclassification/image_retrieval\n"
     ]
    }
   ],
   "source": [
    "%cd image_retrieval/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_local=\"/home/anlab/Downloads/imageclassification/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5Q3M_ugkcWO"
   },
   "source": [
    "***Lấy file query ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VEXxH4jNo72V"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "def get_file_query():\n",
    "  path= \"gt_files/\"\n",
    "  list_gt = os.listdir(path)\n",
    "  list_files_query=[]\n",
    "  for file_name in list_gt:\n",
    "    name=file_name.split(\".\")[0].split(\"_\")[-1]\n",
    "    if(name==\"query\"):\n",
    "      list_files_query.append(file_name)\n",
    "  return list_files_query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGzmYgTjlFt1"
   },
   "source": [
    "\n",
    "**lấy file ảnh tương ứng với querry (gồm 3 file good, oke, junk)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XDfXSsIszApF"
   },
   "outputs": [],
   "source": [
    "#get file good,oke,junk correspond\n",
    "def get_file_name(list_files_query, name):\n",
    "  list_files_name=[]\n",
    "  for query_file in list_files_query:\n",
    "    name_correspond= query_file.replace(\"query\",name)\n",
    "    list_files_name.append(name_correspond)\n",
    "  return list_files_name\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfqPHLJMlTs3"
   },
   "source": [
    "\n",
    "\n",
    "**lấy ảnh trong file query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Kymc5gp21yB9"
   },
   "outputs": [],
   "source": [
    "def get_image_query(list_files_query):\n",
    "  list_image_query=[]\n",
    "  for query in list_files_query:\n",
    "    dir= path_local+\"/image_retrieval/gt_files/\"+query\n",
    "    with open(dir,\"r\") as f:\n",
    "      for line in f.readlines():\n",
    "        name_image= line.split(\" \")[0].split(\"_\",1)[1]+\".jpg\"\n",
    "        list_image_query.append(name_image)\n",
    "  return list_image_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Grj5Gnlrlar7"
   },
   "source": [
    "**lấy ảnh tương ứng của query với từng file good, oke, junk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-tu809Pl6ypj"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_image_correspond(file_name):\n",
    "  list_image_name= []\n",
    "  with open(\"gt_files/\"+file_name,\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "      list_image_name.append(line.split(\"\\n\")[0]+\".jpg\")\n",
    "  return list_image_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Rmac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/anlab/Downloads/imageclassification/image_retrieval/keras_rmac\n"
     ]
    }
   ],
   "source": [
    "%cd keras_rmac/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "from keras.layers import Lambda, Dense, TimeDistributed, Input\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "import keras.backend as K\n",
    "import keras\n",
    "# from vgg16 import VGG16\n",
    "# from RoiPooling import RoiPooling\n",
    "from PIL import Image,ImageOps\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import utils\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "def rzac(x, L=3, eps=1e-6):\n",
    "#     N, C, H, W = x.size()\n",
    "    feature_total= []\n",
    "    ovr = 0.4 # desired overlap of neighboring regions\n",
    "    steps = torch.Tensor([2, 3, 4, 5, 6, 7]) # possible regions for the long dimension\n",
    "\n",
    "    W = x.size(3)\n",
    "    H = x.size(2)\n",
    "\n",
    "    w = min(W, H)\n",
    "    w2 = math.floor(w/2.0 - 1)\n",
    "\n",
    "    b = (max(H, W)-w)/(steps-1)\n",
    "    (tmp, idx) = torch.min(torch.abs(((w**2 - w*b)/w**2)-ovr), 0) # steps(idx) regions for long dimension\n",
    "\n",
    "    # region overplus per dimension\n",
    "    Wd = 0;\n",
    "    Hd = 0;\n",
    "    if H < W:  \n",
    "        Wd = idx.item() + 1\n",
    "    elif H > W:\n",
    "        Hd = idx.item() + 1\n",
    "\n",
    "    v = F.max_pool2d(x, (x.size(-2), x.size(-1)))\n",
    "    v = v / (torch.norm(v, p=2, dim=1, keepdim=True) + eps).expand_as(v)\n",
    "    \n",
    "    for l in range(1, L+1):\n",
    "        \n",
    "        wl = math.floor(2*w/(l+1))\n",
    "        wl2 = math.floor(wl/2 - 1)\n",
    "        \n",
    "        if l+Wd == 1:\n",
    "            b = 0\n",
    "        else:\n",
    "            b = (W-wl)/(l+Wd-1)\n",
    "        cenW = torch.floor(wl2 + torch.Tensor(range(l-1+Wd+1))*b) - wl2 # center coordinates\n",
    "        if l+Hd == 1:\n",
    "            b = 0\n",
    "        else:\n",
    "            b = (H-wl)/(l+Hd-1)\n",
    "        cenH = torch.floor(wl2 + torch.Tensor(range(l-1+Hd+1))*b) - wl2 # center coordinates\n",
    "        \n",
    "        for i_ in cenH.tolist():\n",
    "            for j_ in cenW.tolist():\n",
    "                if wl == 0:\n",
    "                    continue\n",
    "                R = x[:,:,(int(i_)+torch.Tensor(range(wl)).long()).tolist(),:]\n",
    "                R = R[:,:,:,(int(j_)+torch.Tensor(range(wl)).long()).tolist()]\n",
    "                vt = F.max_pool2d(R, (R.size(-2), R.size(-1)))\n",
    "                vt = vt / (torch.norm(vt, p=2, dim=1, keepdim=True) + eps).expand_as(vt)\n",
    "#                 v += vt\n",
    "                feature_total.append(vt)\n",
    "    feature_total=np.vstack(feature_total)\n",
    "    feature_total=np.squeeze(feature_total,axis=2)\n",
    "    feature_total= np.transpose(feature_total,(2,0,1))\n",
    "    feature_total=torch.from_numpy(feature_total)\n",
    "    return feature_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(reg_feat_mat):\n",
    "  reg_feat_mat = reg_feat_mat.squeeze() # [1, r, c] -> [r, c]\n",
    "#   r = reg_feat_mat.size(0)\n",
    "  ag_feat_vec = torch.sum(reg_feat_mat, dim=0, keepdim=False) # (r, c) -> (c,)\n",
    "  ag_feat_vec = ag_feat_vec / (torch.norm(ag_feat_vec,\n",
    "                                            p=2,\n",
    "                                            dim=0,\n",
    "                                            keepdim=True) + 1e-6)\n",
    "  ag_feat_vec=np.array(ag_feat_vec).reshape(1,-1)\n",
    "  return ag_feat_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "def load_image_and_bbs(im_path, bb_path):\n",
    "    img = cv2.imread(im_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    bb_df = pd.read_csv(bb_path, sep=' ', header=None, index_col=False)\n",
    "    bb_mat = bb_df.to_numpy()\n",
    "    return img, bb_mat\n",
    "\n",
    "def _get_masked_img( img, bbs):\n",
    "    patches = []\n",
    "#     masked = np.zeros_like(img)\n",
    "    for bb in bbs:\n",
    "\n",
    "        x_l = bb[0]\n",
    "        x_r = bb[0] + bb[2]\n",
    "        y_u = bb[1]\n",
    "        y_d = bb[1] + bb[3]\n",
    "        patches.append(img[y_u:y_d, x_l:x_r])\n",
    "#         masked[y_u:y_d, x_l:x_r] = img[y_u:y_d, x_l:x_r]\n",
    "\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "list_files_query=os.listdir(path_local+\"data/pg_data/query\")\n",
    "from shutil import copyfile\n",
    "path=path_local+\"data/pg_data/\"\n",
    "for i in range(len(list_files_query)):\n",
    "  name_file=list_files_query[i]\n",
    "  with open(path+\"query/\"+name_file,\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "      line= line.split(\" \",1)\n",
    "      name_images=line[0].split(\"_\",1)[1]+\".jpg\"\n",
    "      coordinate= line[1].split(\"\\n\")[0].split(\" \")\n",
    "      coordinate=[round(float(value)) for value in coordinate]\n",
    "      coordinate=str(coordinate[0])+\" \"+str(coordinate[1])+\" \"+str(coordinate[2]-coordinate[0])+\" \"+str(coordinate[3]-coordinate[1])\n",
    "      if(i+1<10):\n",
    "        copyfile(path+\"Images/\"+name_images, path+\"Queries/\"+f\"0{i+1}.jpg\")\n",
    "        with open(path+\"Queries/\"+f\"0{i+1}.txt\",\"w\") as f:\n",
    "          f.write(coordinate)\n",
    "      else :\n",
    "        copyfile(path+\"Images/\"+name_images, path+\"Queries/\"+f\"{i+1}.jpg\")\n",
    "        with open(path+\"Queries/\"+f\"{i+1}.txt\",\"w\") as f:\n",
    "          f.write(coordinate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "query_dir = path_local+'data/pg_data/Queries'\n",
    "im_bb_path_tuples = [(os.path.join(query_dir, '{:02d}.jpg'.format(i)),\n",
    "                      os.path.join(query_dir, '{:02d}.txt'.format(i))\n",
    "                     ) for i in range(1, 56)\n",
    "                    ]\n",
    "len(im_bb_path_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mac(x):\n",
    "    vt= F.max_pool2d(x, (x.size(-2), x.size(-1)))\n",
    "    vt=np.squeeze(vt,axis=2)\n",
    "    vt= np.transpose(vt,(2,0,1))\n",
    "    return vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:03<00:00, 14.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#query mac\n",
    "\n",
    "import math\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "model= keras.applications.VGG16(include_top=False,weights=utils.DATA_DIR + utils.WEIGHTS_FILE)\n",
    "vector_query =[]\n",
    "for value in tqdm(im_bb_path_tuples):\n",
    "    demo_im_path, demo_bb_path = im_bb_path_tuples[0]\n",
    "    demo_im, demo_bb = load_image_and_bbs(demo_im_path, demo_bb_path)\n",
    "    masked=_get_masked_img(demo_im, demo_bb)\n",
    "    img=masked[0]\n",
    "    x = change_chanel(img)\n",
    "    im_tensor = model.predict(x)\n",
    "    im_tensor=torch.tensor(im_tensor)\n",
    "    reg_feat_mat= mac(im_tensor)\n",
    "    MAC=reg_feat_mat = reg_feat_mat.squeeze(0) # [1, r, c] -> [r, c]\n",
    "    vector_query.append(MAC[-1])\n",
    "print(len(vector_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query rmac\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# vector_query =[]\n",
    "\n",
    "# for value in tqdm(im_bb_path_tuples):\n",
    "    \n",
    "#     demo_im_path, demo_bb_path = value\n",
    "#     demo_im, demo_bb = load_image_and_bbs(demo_im_path, demo_bb_path)\n",
    "#     masked=_get_masked_img(demo_im, demo_bb)\n",
    "#     x= np.moveaxis(masked[0], -1, 0)\n",
    "#     x=np.expand_dims(x, axis=0)\n",
    "#     im_tensor=model.predict(x)\n",
    "#     im_tensor=torch.tensor(im_tensor)\n",
    "#     reg_feat_mat= rzac(im_tensor)\n",
    "# #     print(\"reg_feat_mat\",reg_feat_mat.shape)\n",
    "#     RMAC=get_feature(reg_feat_mat)\n",
    "#     vector_query.append(RMAC[-1])\n",
    "    \n",
    "# print(len(vector_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_Db(reg_feat_mat):\n",
    "    reg_feat_mat = reg_feat_mat.squeeze() # [1, r, c] -> [r, c]\n",
    "    return reg_feat_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_chanel(tensor_img):\n",
    "    x= np.moveaxis(tensor_img, -1, 0)\n",
    "    x=np.expand_dims(x, axis=0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessImage(linkImage):\n",
    "    img=cv2.imread(linkImage)\n",
    "#     img=cv2.resize(img,(724,724))\n",
    "    x = change_chanel(img)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Rmac(file,model):\n",
    "    x = preprocessImage(file)\n",
    "    #get feature each tensor image\n",
    "    im_tensor = model.predict(x)\n",
    "    #conver to torch tensor\n",
    "    im_tensor=torch.tensor(im_tensor)\n",
    "    #caculate rzac\n",
    "    reg_feat_mat= rzac(im_tensor)   \n",
    "    #caculate rmac \n",
    "    RMAC = get_feature(reg_feat_mat)\n",
    "    return RMAC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Rmac_DB(file,model):\n",
    "    x = preprocessImage(file)\n",
    "    #get feature each tensor image\n",
    "    im_tensor = model.predict(x)\n",
    "    #conver to torch tensor\n",
    "    im_tensor=torch.tensor(im_tensor)\n",
    "    #caculate rzac\n",
    "    reg_feat_mat= rzac(im_tensor)\n",
    "    nVector_represent=get_feature_Db(reg_feat_mat)\n",
    "    return nVector_represent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/anlab/Downloads/imageclassification/image_retrieval\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/anlab/Downloads/imageclassification/image_retrieval/keras_rmac\n"
     ]
    }
   ],
   "source": [
    "%cd keras_rmac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "# get file image correspond with query\n",
    "im_list_query=get_image_query(list_files_query)\n",
    "print(len(im_list_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dataset.pickle',\"rb\") as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5063/5063 [20:00<00:00,  4.22it/s] \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "with open('dataset.pickle',\"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "dataset_new=[]\n",
    "vector_dataset=[]\n",
    "for img in tqdm(dataset):\n",
    "    file = path_local+\"Object Dataset/test/oxbuild_images/\"+img\n",
    "    # Load RMAC model\n",
    "    RMAC_DB=get_Rmac_DB(file,model)\n",
    "    vector_dataset.append(RMAC_DB)\n",
    "    for i in range(RMAC_DB.shape[0]):\n",
    "        dataset_new.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anlab/anaconda3/envs/myenvs/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# with open('dataset_new.pickle', 'wb') as f:\n",
    "#     pickle.dump(dataset_new, f)\n",
    "# with open('vector_dataset.pickle', 'wb') as f:\n",
    "#     pickle.dump(vector_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.pickle',\"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "with open('dataset_new.pickle',\"rb\") as f:\n",
    "    dataset_new = pickle.load(f)\n",
    "with open('vector_dataset.pickle',\"rb\") as f:\n",
    "    vector_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102106"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102106, 512)\n"
     ]
    }
   ],
   "source": [
    "vector_dataset=np.vstack(vector_dataset)\n",
    "print(vector_dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55, 512)\n"
     ]
    }
   ],
   "source": [
    "vector_query_test=[t.numpy() for t in vector_query]\n",
    "vector_query=np.array(vector_query_test)\n",
    "print(vector_query.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tính giao của 2 array-> kết quả trả về theo thứ tự của array 1\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/anlab/Downloads/imageclassification/image_retrieval\n"
     ]
    }
   ],
   "source": [
    "%cd /home/anlab/Downloads/imageclassification/image_retrieval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files_good=get_file_name(list_files_query,\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102106"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk=len(dataset_new)\n",
    "topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def result_image_query(vector_query_x, topk):\n",
    "    \n",
    "    #distance query_x with datasets\n",
    "    vector_l2_dataset= np.linalg.norm(vector_query_x- vector_dataset ,axis=-1)\n",
    "    \n",
    "    #sort index image\n",
    "    toplen_image_query= np.argsort(vector_l2_dataset)\n",
    "    \n",
    "    image_correspond = [dataset_new[image] for image in toplen_image_query]\n",
    "    image_correspond = list(OrderedDict.fromkeys(image_correspond))\n",
    "    image_topk= image_correspond[:topk]\n",
    "    \n",
    "    return image_topk\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ashmolean_000106.jpg',\n",
       " 'ashmolean_000000.jpg',\n",
       " 'ashmolean_000028.jpg',\n",
       " 'oxford_000904.jpg',\n",
       " 'oxford_000575.jpg']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test search image \n",
    "import math\n",
    "# model= keras.applications.VGG16(include_top=False,weights=utils.DATA_DIR + utils.WEIGHTS_FILE)\n",
    "file = path_local+\"Object Dataset/test/oxbuild_images/\"+\"ashmolean_000106.jpg\"\n",
    "x = preprocessImage(file)\n",
    "im_tensor = model.predict(x)\n",
    "im_tensor=torch.tensor(im_tensor)\n",
    "feature=rzac(im_tensor)\n",
    "rmac_file_test=get_feature(feature)\n",
    "\n",
    "\n",
    "image_topk= result_image_query(rmac_file_test,5)\n",
    "image_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def map_name(list_files_query,name,vector_query,vector_dataset,dataset_new,topk):\n",
    "  list_ap_name=[]\n",
    "  list_files_name=get_file_name(list_files_query,name)\n",
    "    \n",
    "  for i in range(len(list_files_query)):\n",
    "    #get grount truth\n",
    "    gt=get_image_correspond(list_files_name[i])\n",
    "    \n",
    "    image_correspond = result_image_query(vector_query[i],topk)\n",
    "    index=[]\n",
    "    AP=0\n",
    "    for j in range(len(image_correspond)):\n",
    "      if(image_correspond[j] in gt):\n",
    "        index.append(j)\n",
    "        \n",
    "    for k in range(len(index)):\n",
    "      AP= AP+ ((k+1)/(index[k]+1))\n",
    "    AP=AP/len(gt)\n",
    "    list_ap_name.append(AP)\n",
    "    \n",
    "  map_name=sum(list_ap_name)/len(list_ap_name)\n",
    "\n",
    "  return map_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_good= map_name(list_files_query,\"good\",vector_query,vector_dataset,dataset_new,topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07715097784412402\n"
     ]
    }
   ],
   "source": [
    "print(map_good)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO6YAqcbq+vm2+IagH8h+4A",
   "collapsed_sections": [],
   "name": "image_retrieval.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
